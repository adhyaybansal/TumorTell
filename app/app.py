# Import necessary packages from libraries
import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.metrics import Precision, Recall

import numpy as np
import plotly.graph_objects as go
import cv2
import base64

from groq import Groq
import google.generativeai as genai
from PIL import Image
import os
from dotenv import load_dotenv


load_dotenv()

# Retrieve Tokens from environment variables
ngrok_api_token = '2vzeP8B34yOq7smUGquekkeohmF_5cAgZZFgaxgpFxTQSpbFG'
gemini_api_token = 'AIzaSyBUiUu5fgaKgirrOS7cXU4iEfR7jj-bmdQ'
hf_api_token = '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'
groq_api_token = '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'

# Retrieve Paths to model_weights
xception_path = "D:/RESEARCH 6TH SEM/xception_model.weights.h5"
custom_cnn_path = "D:/RESEARCH 6TH SEM/custom_cnn_model.weights.h5"
augmented_model_path = "D:/RESEARCH 6TH SEM/data_augmented_cnn_model_final.h5"

# Create saliency map directory for storing saliency maps
output_dir = 'saliency_maps'
os.makedirs(output_dir, exist_ok=True)

# Configure LLMs for generating explanations
genai.configure(api_key=gemini_api_token)

def create_prompt_based_on_user_type(user_type, model_prediction, confidence):
    if user_type == 'Doctor / Medical Expert':
        prompt = f"""You are an expert neurologist reviewing a brain MRI saliency map.
        The saliency map was generated by a deep learning model that was trained to classify brain tumors
        as either glioma, meningioma, no tumor, or pituitary.

        The deep learning model predicted the image to be of class '{model_prediction}' with confidence = {confidence * 100:.2f}%.

        In your response: 
        - Explain the medical significance using professional terminology.
        - Explain what regions of the brain the model is focusing on, based on the saliency map. Refer to the areas highlighted
        in light cyan as potential tumor regions, referencing possible pathology.
        - Explain possible reasons why the model made the prediction it did.
        - Don't mention anything like 'The saliency map highlights the regions the model is focusing on, which are in light cyan'
        or 'The model shows x% probability of...' or anything percentages. 
        Just state your understanding of the facts presented. No need to state the numerical vaalues.
        - Suggest next diagnostic steps an expert would consider.
        - Keep it concise (under 8 sentences).

        Let's think about this, step by step. Verify and analyze step by step that your explanation is correct.
        """
    else:
        prompt = f"""You are an expert neuro-surgeon reviewing a brain MRI saliency map.
        The saliency map was generated by a deep learning model that was trained to classify brain tumors
        as either glioma, meningioma, no tumor, or pituitary.

        The deep learning model predicted the image to be of class '{model_prediction}' with confidence = {confidence * 100:.2f}%.

        In your response: 
        - Use everyday language to describe what that might mean for their health.
        - Point out the highlighted areas as "spots the model is worried about." (highlighted in light cyan color)
        - Briefly suggest what the patient's next steps could be, such as talking to a specialist or getting more tests and so on.
        - Don't mention anything like 'The saliency map highlights the regions the model is focusing on, which are in light cyan',
        or 'The model shows x% probability of...' or use percentages in your explanation. 
        Just give an explanation based on the facts presented. No need to state the facts.
        - Keep it under six sentences and avoid heavy technical jargon.

        Let's think about this, step by step. Verify and analyze step by step that your explanation is correct.
        """
    
    return prompt

def generate_explanation_by_gemini(image_path, prompt):

    saliency_image = Image.open(image_path)
    model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest')
    response = model.generate_content([prompt, saliency_image])

    return response.text

def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

def generate_explanation_by_groq_api(image_path, prompt):
    client = Groq(api_key=groq_api_token)
    base64_image = encode_image(image_path)
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                            },
                        },
                    ],
                }
            ],
            model="mixtral-8x7b-32768-v0.1",  # Use a supported model
        )
        result = chat_completion.choices[0].message.content
        print(result)
        return result
    except Exception as e:
        st.error(f"Error communicating with Groq API: {e}")
        return "An error occurred while generating the explanation."

def generate_explanation(image_path, prompt, llm_choice):
    if llm_choice == "Google Gemini-1.5 Flash (Latest)":
        return generate_explanation_by_gemini(image_path, prompt)
    elif llm_choice == "Llama 3.2 11B Vision (Preview)":
        return generate_explanation_by_groq_api(image_path, prompt)
    else:
        st.error("Invalid LLM choice.")
        return "An error occurred while generating the explanation."

def generate_saliency_map(model, input_image, image_array, class_index, image_size, uploaded_file):
    with tf.GradientTape() as tape:
        image_tensor = tf.convert_to_tensor(image_array)
        tape.watch(image_tensor)
        predictions = model(image_tensor)
        target_class = predictions[:, class_index]
    
    gradients = tape.gradient(target_class, image_tensor)
    gradients = tf.math.abs(gradients)
    gradients = tf.reduce_max(gradients, axis=-1)
    gradients = gradients.numpy().squeeze()

    # Resize the gradients to match original image size
    gradients = cv2.resize(gradients, image_size)

    # Create circular mask for brain area
    center = (gradients.shape[0] // 2, gradients.shape[1] // 2)
    radius = min(center[0], center[1]) - 10
    y, x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]
    mask = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= radius ** 2

    # Apply mask to gradients
    gradients *= mask

    # Normalize the brain area
    brain_gradients = gradients[mask]
    if brain_gradients.max() > brain_gradients.min():
        brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max() - brain_gradients.min())
    gradients[mask] = brain_gradients

    # Apply a higher threshold
    threshold = np.percentile(gradients[mask], 80)
    gradients[gradients < threshold] = 0

    # Apply an aggresive smoothing
    gradients = cv2.GaussianBlur(gradients, (11, 11), 0)

    # Heatmap overlay with enhanced contrast
    heatmap = cv2.applyColorMap(np.uint8(255 * gradients), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

    # Resize heatmap to match orginal image size
    heatmap = cv2.resize(heatmap, image_size)

    # Superimpose the heatmap onto the original image
    original_image = image.img_to_array(input_image)
    superimposed_image = heatmap * 0.7 + original_image * 0.3
    superimposed_image = superimposed_image.astype(np.uint8)

    image_path = os.path.join(output_dir, uploaded_file.name)
    with open(image_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())

    
    saliency_map_path = f'saliency_maps/{uploaded_file.name}'

    # Save the saliency map
    cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_image, cv2.COLOR_RGB2BGR))

    return superimposed_image


def load_xception_model(weights_path: str):
    img_shape = (299,299,3)
    base_model = tf.keras.applications.Xception(include_top=False,
                                                weights='imagenet',
                                                input_shape=img_shape,
                                                pooling='max')
    
    model = Sequential([
        base_model,
        Flatten(),
        Dropout(rate=0.3),
        Dense(128, activation='relu'),
        Dropout(rate=0.25),
        Dense(4, activation='softmax')
    ])
    model.build((None,) + img_shape)

    # Compile the model
    model.compile(Adamax(learning_rate=0.001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy', Precision(), Recall()])
    
    model.load_weights(weights_path)
    return model


# Streamlit UI
def main():
    # Set the page configuration
    st.set_page_config(
        page_title="TumorTell",
        
        layout="centered"
    )
    st.markdown("""
    <style>
        body {
            background-color: black;
            color: white;
        }
        .stApp {
            background-color: black;
        }
        header {
            background-color: red !important;
        }
        [data-testid="stHeader"] {
            background-color: red !important;
        }
    </style>
""", unsafe_allow_html=True)
    st.title("Brain Tumor Classification")
    st.write("Upload an MRI scan image to examine if it contains a tumor.")
    
    # File uploader section
    uploaded_file = st.file_uploader(
        "Upload an image file (.jpg, .jpeg, .png)",
        type=["jpg", "jpeg", "png"],
        help="Drag and drop an image or click to browse"
    )
    if uploaded_file is not None:
        # Display the uploaded image
        uploaded_image = Image.open(uploaded_file)
        st.image(uploaded_image, caption="Your Uploaded MRI Scan", use_container_width=True)
        
        # Placeholder for classification logic
        st.success("Image uploaded successfully! The classification will appear below.")

        selected_model = st.radio(
            "Select a Model",
            ('Xception - Transfer Learning', 'Self-Designed CNN', 'Self-Designed Augmented CNN'),
        )
        if selected_model == 'Xception - Transfer Learning':
            model = load_xception_model(xception_path)
            image_size = (299,299)
        
        elif selected_model == 'Self-Designed Augmented CNN':
            model = load_model(augmented_model_path)
            image_size = (224,224)

        else:
            model = load_model(custom_cnn_path) 
            image_size = (224,224)
        

        # Make predictions using loaded models
        labels = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']

        input_image = image.load_img(uploaded_file, target_size=image_size)
        
        input_image_array = image.img_to_array(input_image)
        input_image_array = np.expand_dims(input_image_array, axis=0)
        input_image_array /= 255.0

        prediction = model.predict(input_image_array)

        # Retrieve ouput class with highest probability(confidence score)
        class_index = np.argmax(prediction[0])
        result = labels[class_index]

        st.write(f"Predicted class of tumor: {result}")
        st.write("Predictions:")

        # Display confidence scores
        for label, prob in zip(labels, prediction[0]):
            st.write(f"{label} : {prob:.4f}")

        saliency_map = generate_saliency_map(model, input_image, input_image_array, class_index, image_size, uploaded_file)
        col1, col2 = st.columns(2)
        with col1:
            st.image(uploaded_file, caption='Uploaded MRI Scan', use_container_width=True)
        with col2:
            st.image(saliency_map, caption='Saliency Map', use_container_width=True)

        saliency_map_path = f'saliency_maps/{uploaded_file.name}'
        user_type = st.radio(
            "Who would you most closely identify as?",
            ("Doctor / Medical Expert", "Patient")
        )
        prompt_to_llm = create_prompt_based_on_user_type(user_type, result, prediction[0][class_index])

        llm_choice = st.selectbox(
            "Choose an LLM for Explanation of Results",
            ["Google Gemini-1.5 Flash (Latest)", "Llama 3.2 11B Vision (Preview)"]
        )
        explanation_of_results = generate_explanation(saliency_map_path, prompt_to_llm, llm_choice)
        
        st.write("## Explanation of Results")
        st.write(explanation_of_results)


if __name__ == "__main__":
    main()